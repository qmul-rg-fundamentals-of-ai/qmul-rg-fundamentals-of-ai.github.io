---
---

@inproceedings{mason-williams2024neural,
title={{NEURAL} {NETWORK} {COMPRESSION}: {THE} {FUNCTIONAL} {PERSPECTIVE}},
author={Israel Mason-Williams},
booktitle={5th Workshop on practical ML for limited/low resource settings},
year={2024},
html={https://openreview.net/forum?id=Q7GXKjmCSB}
}

@inproceedings{mason-williams2024knowledge,
title={Knowledge Distillation: The Functional Perspective},
author={Israel Mason-Williams and Gabryel Mason-Williams and Mark Sandler},
booktitle={NeurIPS 2024 Workshop on Scientific Methods for Understanding Deep Learning},
year={2024},
html={https://openreview.net/forum?id=Cgo73ZnAQc}
}

@article{cleeremans1989finite,
  title={Finite state automata and simple recurrent networks},
  author={Cleeremans, Axel and Servan-Schreiber, David and McClelland, James L},
  journal={Neural computation},
  volume={1},
  number={3},
  pages={372--381},
  year={1989},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…},
  html={https://www.researchgate.net/publication/243698906_Finite_State_Automata_and_Simple_Recurrent_Networks}
}

@inproceedings{pavlovic2025understandingmodelcalibration,
  author = {Pavlovic, Maja},
  title = {Understanding Model Calibration - A gentle introduction and visual exploration of calibration and the expected calibration error (ECE)},
  abstract = {To be considered reliable, a model must be calibrated so that its confidence in each decision closely reflects its true outcome. In this blogpost we'll take a look at the most commonly used definition for calibration and then dive into a frequently used evaluation measure for model calibration. We'll then cover some of the drawbacks of this measure and how these surfaced the need for additional notions of calibration, which require their own new evaluation measures. This post is not intended to be an in-depth dissection of all works on calibration, nor does it focus on how to calibrate models. Instead, it is meant to provide a gentle introduction to the different notions and their evaluation measures as well as to re-highlight some  issues with a measure that is still widely used to evaluate calibration.},
  booktitle = {ICLR Blogposts 2025},
  year = {2025},
  date = {April 28, 2025},
  note = {https://iclr-blogposts.github.io/2025/blog/calibration/},
  html  = {https://iclr-blogposts.github.io/2025/blog/calibration/}
}

@misc{wałęga2024expressivepowertemporalmessage,
      title={Expressive Power of Temporal Message Passing}, 
      author={Przemysław Andrzej Wałęga and Michael Rawson},
      year={2024},
      eprint={2408.09918},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      html={https://arxiv.org/abs/2408.09918}, 
}

@article{Wałęga_Rawson_2025, 
  title={Expressive Power of Temporal Message Passing}, 
  volume={39}, 
  url={https://ojs.aaai.org/index.php/AAAI/article/view/35396}, 
  DOI={10.1609/aaai.v39i20.35396}, 
  abstractNote={Graph neural networks (GNNs) have recently been adapted to temporal settings, often employing temporal versions of the message-passing mechanism known from GNNs. We divide temporal message passing mechanisms from literature into two main types: global and local, and establish Weisfeiler-Leman characterisations for both. This allows us to formally analyse expressive power of temporal message-passing models. We show that global and local temporal message-passing mechanisms have incomparable expressive power when applied to arbitrary temporal graphs. However, the local mechanism is strictly more expressive than the global mechanism when applied to colour-persistent temporal graphs, whose node colours are initially the same in all time points. Our theoretical findings are supported by experimental evidence, underlining practical implications of our analysis.}, 
  number={20}, 
  journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
  author={Wałęga, Przemysław Andrzej and Rawson, Michael}, 
  year={2025}, 
  month={Apr.}, 
  pages={21000-21008} 
}

@misc{yunis2024approachingdeeplearningspectral,
      title={Approaching Deep Learning through the Spectral Dynamics of Weights}, 
      author={David Yunis and Kumar Kshitij Patel and Samuel Wheeler and Pedro Savarese and Gal Vardi and Karen Livescu and Michael Maire and Matthew R. Walter},
      year={2024},
      eprint={2408.11804},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      html={https://arxiv.org/abs/2408.11804}, 
}

@article{mante2013context,
  title={Context-dependent computation by recurrent dynamics in prefrontal cortex},
  author={Mante, Valerio and Sussillo, David and Shenoy, Krishna V and Newsome, William T},
  journal={nature},
  volume={503},
  number={7474},
  pages={78--84},
  year={2013},
  publisher={Nature Publishing Group UK London},
  html={https://www.nature.com/articles/nature12742}
}

@article{zemlianova2024dynamical,
  title={Dynamical mechanisms of how an RNN keeps a beat, uncovered with a low-dimensional reduced model},
  author={Zemlianova, Klavdia and Bose, Amitabha and Rinzel, John},
  journal={Scientific Reports},
  volume={14},
  number={1},
  pages={26388},
  year={2024},
  publisher={Nature Publishing Group UK London},
  html={https://www.nature.com/articles/s41598-024-77849-x}
}

@article{roman2023hebbian,
  title={Hebbian learning with elasticity explains how the spontaneous motor tempo affects music performance synchronization},
  author={Roman, Iran R and Roman, Adrian S and Kim, Ji Chul and Large, Edward W},
  journal={PLOS Computational Biology},
  volume={19},
  number={6},
  pages={e1011154},
  year={2023},
  publisher={Public Library of Science San Francisco, CA USA},
  html={https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1011154}
}

@article{driscoll2024flexible,
  title={Flexible multitask computation in recurrent networks utilizes shared dynamical motifs},
  author={Driscoll, Laura N and Shenoy, Krishna and Sussillo, David},
  journal={Nature Neuroscience},
  volume={27},
  number={7},
  pages={1349--1363},
  year={2024},
  publisher={Nature Publishing Group US New York},
  html={https://www.nature.com/articles/s41593-024-01668-6}
}

@misc{wilson2025deeplearningmysteriousdifferent,
      title={Deep Learning is Not So Mysterious or Different}, 
      author={Andrew Gordon Wilson},
      year={2025},
      eprint={2503.02113},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      html={https://arxiv.org/abs/2503.02113}, 
}

@article{huang2020understanding,
  title={Understanding generalization through visualizations},
  author={Huang, W Ronny and Emam, Zeyad and Goldblum, Micah and Fowl, Liam and Terry, Justin K and Huang, Furong and Goldstein, Tom},
  year={2020},
  publisher={Proceedings of Machine Learning Research},
  html={https://proceedings.mlr.press/v137/huang20a.html}
}

@article{fort2019deep,
  title={Deep ensembles: A loss landscape perspective},
  author={Fort, Stanislav and Hu, Huiyi and Lakshminarayanan, Balaji},
  journal={arXiv preprint arXiv:1912.02757},
  year={2019},
  html={https://arxiv.org/abs/1912.02757}
}

@inproceedings{
mason-williams2024what,
title={What Makes a Good Prune? Maximal Unstructured Pruning for Maximal Cosine Similarity},
author={Gabryel Mason-Williams and Fredrik Dahlqvist},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
html={https://openreview.net/forum?id=jsvvPVVzwf}
}

@article{martin2021predicting,
  title={Predicting trends in the quality of state-of-the-art neural networks without access to training or testing data},
  author={Martin, Charles H and Peng, Tongsu and Mahoney, Michael W},
  journal={Nature Communications},
  volume={12},
  number={1},
  pages={4122},
  year={2021},
  publisher={Nature Publishing Group UK London},
  html={https://www.nature.com/articles/s41467-021-24025-8}
}

@inproceedings{
li2018measuring,
title={Measuring the Intrinsic Dimension of Objective Landscapes},
author={Chunyuan Li and Heerad Farkhoor and Rosanne Liu and Jason Yosinski},
booktitle={International Conference on Learning Representations},
year={2018},
html={https://openreview.net/forum?id=ryup8-WCW},
}